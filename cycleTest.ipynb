{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of Adult dataset experiments\n",
    "\n",
    "In this notebook we reproduce the results from Table 2 of the DECAF paper. We compare various methods for generating debiased data using the DECAF model against synthetic data generated using benchmark models GAN, WGAN-GP and FairGAN. As described in the paper we run all experiments (as implemented in this notebook) 10 times and avarage the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from data import load_adult, preprocess_adult\n",
    "from metrics import DP, FTU\n",
    "from train import train_decaf, train_fairgan, train_vanilla_gan, train_wgan_gp\n",
    "# import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.config.run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age         workclass    fnlwgt  education  education-num  \\\n",
       "0  39         State-gov   77516.0  Bachelors           13.0   \n",
       "1  50  Self-emp-not-inc   83311.0  Bachelors           13.0   \n",
       "2  38           Private  215646.0    HS-grad            9.0   \n",
       "3  53           Private  234721.0       11th            7.0   \n",
       "4  28           Private  338409.0  Bachelors           13.0   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0        2174.0           0.0            40.0  United-States  <=50K  \n",
       "1           0.0           0.0            13.0  United-States  <=50K  \n",
       "2           0.0           0.0            40.0  United-States  <=50K  \n",
       "3           0.0           0.0            40.0  United-States  <=50K  \n",
       "4           0.0           0.0            40.0           Cuba  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_adult()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data next in order to make it suitable for training models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocess_adult(dataset)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test folds. Test fold size is 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     age  workclass    fnlwgt  education  education-num  \\\n",
      "age             1.000000   0.132393 -0.075792   0.121167       0.037623   \n",
      "workclass       0.132393   1.000000 -0.026970   0.031062       0.178857   \n",
      "fnlwgt         -0.075792  -0.026970  1.000000   0.018028      -0.041993   \n",
      "education       0.121167   0.031062  0.018028   1.000000      -0.232239   \n",
      "education-num   0.037623   0.178857 -0.041993  -0.232239       1.000000   \n",
      "marital-status -0.223313  -0.054065  0.027992  -0.012672      -0.099085   \n",
      "occupation      0.028364   0.124357  0.004324   0.015018      -0.042766   \n",
      "relationship    0.119980   0.008361  0.014071   0.032253      -0.032970   \n",
      "race           -0.023444   0.037899  0.106732   0.008541      -0.077987   \n",
      "sex             0.082053  -0.007633  0.027240   0.033126       0.003417   \n",
      "capital-gain    0.079683   0.014562 -0.004110   0.027249       0.126907   \n",
      "capital-loss    0.059351   0.019798 -0.004349   0.011847       0.081711   \n",
      "hours-per-week  0.101992   0.026440 -0.018679   0.007970       0.146206   \n",
      "native-country -0.029583  -0.056982  0.093291   0.166552      -0.169477   \n",
      "income         -0.237040  -0.086802  0.007264  -0.006749      -0.332800   \n",
      "\n",
      "                marital-status  occupation  relationship      race       sex  \\\n",
      "age                  -0.223313    0.028364      0.119980 -0.023444  0.082053   \n",
      "workclass            -0.054065    0.124357      0.008361  0.037899 -0.007633   \n",
      "fnlwgt                0.027992    0.004324      0.014071  0.106732  0.027240   \n",
      "education            -0.012672    0.015018      0.032253  0.008541  0.033126   \n",
      "education-num        -0.099085   -0.042766     -0.032970 -0.077987  0.003417   \n",
      "marital-status        1.000000    0.013685      0.368251  0.130149 -0.381525   \n",
      "occupation            0.013685    1.000000      0.008064  0.043906 -0.040557   \n",
      "relationship          0.368251    0.008064      1.000000  0.119460 -0.178863   \n",
      "race                  0.130149    0.043906      0.119460  1.000000 -0.115724   \n",
      "sex                  -0.381525   -0.040557     -0.178863 -0.115724  1.000000   \n",
      "capital-gain         -0.068491   -0.013344     -0.026037 -0.021074  0.047444   \n",
      "capital-loss         -0.064985   -0.010014     -0.027376 -0.025456  0.046457   \n",
      "hours-per-week       -0.223325    0.039871      0.052393 -0.056131  0.231425   \n",
      "native-country        0.044357    0.004237      0.047951  0.053014  0.006489   \n",
      "income                0.377580    0.042349      0.181754  0.096050 -0.215760   \n",
      "\n",
      "                capital-gain  capital-loss  hours-per-week  native-country  \\\n",
      "age                 0.079683      0.059351        0.101992       -0.029583   \n",
      "workclass           0.014562      0.019798        0.026440       -0.056982   \n",
      "fnlwgt             -0.004110     -0.004349       -0.018679        0.093291   \n",
      "education           0.027249      0.011847        0.007970        0.166552   \n",
      "education-num       0.126907      0.081711        0.146206       -0.169477   \n",
      "marital-status     -0.068491     -0.064985       -0.223325        0.044357   \n",
      "occupation         -0.013344     -0.010014        0.039871        0.004237   \n",
      "relationship       -0.026037     -0.027376        0.052393        0.047951   \n",
      "race               -0.021074     -0.025456       -0.056131        0.053014   \n",
      "sex                 0.047444      0.046457        0.231425        0.006489   \n",
      "capital-gain        1.000000     -0.032102        0.083880       -0.016010   \n",
      "capital-loss       -0.032102      1.000000        0.054195       -0.017692   \n",
      "hours-per-week      0.083880      0.054195        1.000000       -0.019309   \n",
      "native-country     -0.016010     -0.017692       -0.019309        1.000000   \n",
      "income             -0.221034     -0.148687       -0.227199        0.058785   \n",
      "\n",
      "                  income  \n",
      "age            -0.237040  \n",
      "workclass      -0.086802  \n",
      "fnlwgt          0.007264  \n",
      "education      -0.006749  \n",
      "education-num  -0.332800  \n",
      "marital-status  0.377580  \n",
      "occupation      0.042349  \n",
      "relationship    0.181754  \n",
      "race            0.096050  \n",
      "sex            -0.215760  \n",
      "capital-gain   -0.221034  \n",
      "capital-loss   -0.148687  \n",
      "hours-per-week -0.227199  \n",
      "native-country  0.058785  \n",
      "income          1.000000  \n"
     ]
    }
   ],
   "source": [
    "datasetMat=dataset.corr()\n",
    "print(datasetMat)\n",
    "dataSeries = datasetMat.unstack().sort_values()\n",
    "# print(dataSeries[dataSeries!=1][dataSeries>.2])\n",
    "# print(dataSeries[dataSeries!=1][dataSeries<-.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 43222\n",
      "Size of test set: 2000\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and testing sets\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size=2000,\n",
    "                                               stratify=dataset['income'])\n",
    "\n",
    "print('Size of train set:', len(dataset_train))\n",
    "print('Size of test set:', len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the DAG\n",
    "\n",
    "We need to define a DAG which captures the biases of the dataset. As described in the DECAF paper normally a causal discovery algorithm is used. In this notebook we simply copy the DAG which as described in the Zhang et al. paper which is the one also used in the DECAF paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 3], [8, 6], [8, 14], [8, 12], [8, 3], [8, 5], [0, 6], [0, 12], [0, 14], [0, 1], [0, 5], [0, 3], [0, 7], [9, 6], [9, 5], [9, 14], [9, 1], [9, 3], [9, 7], [13, 5], [13, 12], [13, 3], [13, 1], [13, 14], [13, 7], [5, 6], [5, 12], [5, 14], [5, 1], [5, 7], [5, 3], [3, 6], [3, 12], [3, 14], [3, 1], [3, 7], [6, 14], [12, 14], [1, 14], [7, 14]]\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Define DAG for Adult dataset\n",
    "dag = [\n",
    "    ['income', 'education'],\n",
    "    \n",
    "    # Edges from race\n",
    "    ['race', 'occupation'],\n",
    "    ['race', 'income'],\n",
    "    ['race', 'hours-per-week'],\n",
    "    ['race', 'education'],\n",
    "    ['race', 'marital-status'],\n",
    "\n",
    "    # Edges from age\n",
    "    ['age', 'occupation'],\n",
    "    ['age', 'hours-per-week'],\n",
    "    ['age', 'income'],\n",
    "    ['age', 'workclass'],\n",
    "    ['age', 'marital-status'],\n",
    "    ['age', 'education'],\n",
    "    ['age', 'relationship'],\n",
    "    \n",
    "    # Edges from sex\n",
    "    ['sex', 'occupation'],\n",
    "    ['sex', 'marital-status'],\n",
    "    ['sex', 'income'],\n",
    "    ['sex', 'workclass'],\n",
    "    ['sex', 'education'],\n",
    "    ['sex', 'relationship'],\n",
    "    \n",
    "    # Edges from native country\n",
    "    ['native-country', 'marital-status'],\n",
    "    ['native-country', 'hours-per-week'],\n",
    "    ['native-country', 'education'],\n",
    "    ['native-country', 'workclass'],\n",
    "    ['native-country', 'income'],\n",
    "    ['native-country', 'relationship'],\n",
    "    \n",
    "    # Edges from marital status\n",
    "    ['marital-status', 'occupation'],\n",
    "    ['marital-status', 'hours-per-week'],\n",
    "    ['marital-status', 'income'],\n",
    "    ['marital-status', 'workclass'],\n",
    "    ['marital-status', 'relationship'],\n",
    "    ['marital-status', 'education'],\n",
    "    \n",
    "    # Edges from education\n",
    "    ['education', 'occupation'],\n",
    "    ['education', 'hours-per-week'],\n",
    "    ['education', 'income'],\n",
    "    ['education', 'workclass'],\n",
    "    ['education', 'relationship'],\n",
    "    \n",
    "    # All remaining edges\n",
    "    ['occupation', 'income'],\n",
    "    ['hours-per-week', 'income'],\n",
    "    ['workclass', 'income'],\n",
    "    ['relationship', 'income']\n",
    "\n",
    "    # #added edges\n",
    "    # ['income', 'education'],\n",
    "    # ['education', 'race']\n",
    "]\n",
    "\n",
    "def dag_to_idx(df, dag):\n",
    "    \"\"\"Convert columns in a DAG to the corresponding indices.\"\"\"\n",
    "\n",
    "    dag_idx = []\n",
    "    for edge in dag:\n",
    "        dag_idx.append([df.columns.get_loc(edge[0]), df.columns.get_loc(edge[1])])\n",
    "\n",
    "    return dag_idx\n",
    "\n",
    "# Convert the DAG to one that can be provided to the DECAF model\n",
    "dag_seed = dag_to_idx(dataset, dag)\n",
    "print(dag_seed)\n",
    "print(len(dag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 12, 14, 1, 5, 3, 7], [14], [-1], [6, 12, 14, 1, 7], [-1], [6, 12, 14, 1, 7, 3], [14], [14], [6, 14, 12, 3, 5], [6, 5, 14, 1, 3, 7], [-1], [-1], [14], [5, 12, 3, 1, 14, 7], [3]]\n"
     ]
    }
   ],
   "source": [
    "maxNode=0\n",
    "for edge in dag_seed:\n",
    "    if max(edge)>maxNode:\n",
    "        maxNode=max(edge)\n",
    "    \n",
    "dag_adj= [[-1]]*(maxNode+1)\n",
    "dagStart=set()\n",
    "for edge in dag_seed:\n",
    "    if edge[0] not in dagStart:\n",
    "        dagStart.add(edge[0])\n",
    "        dag_adj[edge[0]]=[edge[1]]\n",
    "    else:\n",
    "        dag_adj[edge[0]].append(edge[1])\n",
    "print(dag_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order=[]\n",
    "\n",
    "# def findCycle(node, visited, stack):\n",
    "#     order.append(node)\n",
    "#     visited[node] = True\n",
    "#     stack[node]=True\n",
    "#     if dag_adj[node]==[-1]:\n",
    "#         stack[node]=False\n",
    "#         return (False, -1)\n",
    "#     for adj in dag_adj[node]:\n",
    "#         if visited[adj]==False:\n",
    "#             if findCycle(adj, visited, stack)[0]:\n",
    "#                 return (True, stack, visited)\n",
    "#         elif stack[adj]==True:\n",
    "#             return (True, 1)\n",
    "#     stack[node]=False\n",
    "#     return (False,0)\n",
    "\n",
    "# def checkNodes(dag_adj):\n",
    "#     visited = [False]*len(dag_adj)\n",
    "#     stack = [False]*len(dag_adj)\n",
    "#     for node in range(len(dag_adj)):\n",
    "#         if not visited[node]:\n",
    "#             output = findCycle(node, visited, stack)\n",
    "#             if output[0]:\n",
    "#                 return output\n",
    "#     return False\n",
    "\n",
    "# print(checkNodes(dag_adj))\n",
    "# print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapse Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(15, 15), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 13:50:35.787213: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-12-08 13:50:35.788152: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#CODE TO DETECT DIFFERENCES BETWEEN 2 Dense graphs for debugging\n",
    "a=tf.constant([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) #a with 2 cycles, 2 new one values\n",
    "\n",
    "b= tf.constant([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) #b without cycle\n",
    "\n",
    "print(a-b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also necessary to define edges we want to remove from the DAG in order to meet the various fairness criteria described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias dict FTU: {14: [9]}\n",
      "Bias dict DP: {14: [6, 12, 5, 3, 9, 1, 7]}\n",
      "Bias dict CF: {14: [5, 9]}\n"
     ]
    }
   ],
   "source": [
    "def create_bias_dict(df, edge_map):\n",
    "    \"\"\"\n",
    "    Convert the given edge tuples to a bias dict used for generating\n",
    "    debiased synthetic data.\n",
    "    \"\"\"\n",
    "    bias_dict = {}\n",
    "    for key, val in edge_map.items():\n",
    "        bias_dict[df.columns.get_loc(key)] = [df.columns.get_loc(f) for f in val]\n",
    "    \n",
    "    return bias_dict\n",
    "\n",
    "# Bias dictionary to satisfy FTU\n",
    "bias_dict_ftu = create_bias_dict(dataset, {'income': ['sex']})\n",
    "print('Bias dict FTU:', bias_dict_ftu)\n",
    "\n",
    "# Bias dictionary to satisfy DP\n",
    "bias_dict_dp = create_bias_dict(dataset, {'income': [\n",
    "    'occupation', 'hours-per-week', 'marital-status', 'education', 'sex',\n",
    "    'workclass', 'relationship']})\n",
    "print('Bias dict DP:', bias_dict_dp)\n",
    "\n",
    "# Bias dictionary to satisfy CF\n",
    "bias_dict_cf = create_bias_dict(dataset, {'income': [\n",
    "    'marital-status', 'sex']})\n",
    "print('Bias dict CF:', bias_dict_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "We have loaded and preprocessed the data and we are ready to run the experiments. For each experiment we train a generative model, sample synthetic data from the trained model and then obtain metrics by training and evaluating a downstream multi-layer perceptron using the test fold we generated in the previous section. We use the MLP model from `sklearn` with default parameters which matches the settings described in Appendix D of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataset_train, dataset_test):\n",
    "    \"\"\"Helper function that prints evaluation metrics.\"\"\"\n",
    "\n",
    "    X_train, y_train = dataset_train.drop(columns=['income']), dataset_train['income']\n",
    "    X_test, y_test = dataset_test.drop(columns=['income']), dataset_test['income']\n",
    "\n",
    "    clf = MLPClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    dp = DP(clf, X_test)\n",
    "    ftu = FTU(clf, X_test)\n",
    "\n",
    "    return {'precision': precision, 'recall': recall, 'auroc': auroc,\n",
    "            'dp': dp, 'ftu': ftu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original dataset\n",
    "\n",
    "As a benchmark we want to first train the downstream model on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we train various models in order to reproduce the results from Table 2 of the DECAF paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synth_data = train_vanilla_gan(dataset_train)\n",
    "# synth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(synth_data, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synth_data = train_wgan_gp(dataset_train)\n",
    "# synth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(synth_data, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FairGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synth_data = train_fairgan(dataset_train)\n",
    "# synth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(synth_data, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECAF-ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:175: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n",
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:170: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised adjacency matrix as parsed:\n",
      " Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'h_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m synth_data \u001b[39m=\u001b[39m train_decaf(dataset_train, dag_seed)\n\u001b[1;32m      2\u001b[0m synth_data\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/UvA_FACT2022/train.py:176\u001b[0m, in \u001b[0;36mtrain_decaf\u001b[0;34m(train_dataset, dag_seed, biased_edges, h_dim, lr, batch_size, lambda_privacy, lambda_gp, d_updates, alpha, rho, weight_decay, grad_dag_loss, l1_g, l1_W, p_gen, use_mask, epochs, model_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m dm \u001b[39m=\u001b[39m DataModule(train_dataset\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m    157\u001b[0m model \u001b[39m=\u001b[39m DECAF(\n\u001b[1;32m    158\u001b[0m     dm\u001b[39m.\u001b[39mdims[\u001b[39m0\u001b[39m],\n\u001b[1;32m    159\u001b[0m     dag_seed\u001b[39m=\u001b[39mdag_seed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     use_mask\u001b[39m=\u001b[39muse_mask,\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 176\u001b[0m model\u001b[39m.\u001b[39;49mdetangle_graph()\n\u001b[1;32m    177\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39mepochs, logger\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    178\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, dm) \u001b[39m#DOESNT WORK WITH CYCLIC\u001b[39;00m\n",
      "File \u001b[0;32m~/UvA_FACT2022/models/DECAF.py:338\u001b[0m, in \u001b[0;36mDECAF.detangle_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m#collapse cycles to create new dag\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdag_seed \u001b[39m=\u001b[39m [[\u001b[39m8\u001b[39m, \u001b[39m6\u001b[39m], [\u001b[39m8\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m8\u001b[39m, \u001b[39m12\u001b[39m], [\u001b[39m8\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m8\u001b[39m, \u001b[39m5\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m6\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m12\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m7\u001b[39m], [\u001b[39m9\u001b[39m, \u001b[39m6\u001b[39m], [\u001b[39m9\u001b[39m, \u001b[39m5\u001b[39m], [\u001b[39m9\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m9\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m9\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m9\u001b[39m, \u001b[39m7\u001b[39m], [\u001b[39m13\u001b[39m, \u001b[39m5\u001b[39m], [\u001b[39m13\u001b[39m, \u001b[39m12\u001b[39m], [\u001b[39m13\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m13\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m13\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m13\u001b[39m, \u001b[39m7\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m12\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m7\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m6\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m12\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m7\u001b[39m], [\u001b[39m6\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m12\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m1\u001b[39m, \u001b[39m14\u001b[39m], [\u001b[39m7\u001b[39m, \u001b[39m14\u001b[39m]]\n\u001b[1;32m    335\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator \u001b[39m=\u001b[39m Generator_causal(\n\u001b[1;32m    336\u001b[0m     z_dim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_dim,\n\u001b[1;32m    337\u001b[0m     x_dim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_dim,\n\u001b[0;32m--> 338\u001b[0m     h_dim\u001b[39m=\u001b[39mh_dim,\n\u001b[1;32m    339\u001b[0m     use_mask\u001b[39m=\u001b[39muse_mask,\n\u001b[1;32m    340\u001b[0m     dag_seed\u001b[39m=\u001b[39mdag_seed,\n\u001b[1;32m    341\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h_dim' is not defined"
     ]
    }
   ],
   "source": [
    "synth_data = train_decaf(dataset_train, dag_seed)\n",
    "synth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8651851851851852,\n",
       " 'recall': 0.776595744680851,\n",
       " 'auroc': 0.7048301304049416,\n",
       " 'dp': 0.29788303,\n",
       " 'ftu': 0.109999955}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(synth_data, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECAF-FTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:175: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n",
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:170: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised adjacency matrix as parsed:\n",
      " Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üò° M after fci stuff Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üò∂‚Äçüå´Ô∏è self.gen.M right after init Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "MODEL BEFPRE Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "MODEL Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üü° DENSE DAGA Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "ü•∂ generator M Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üü† M AT GEN SYNTHETIC, Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üî¥ GEN ORDER [0, 2, 4, 8, 9, 10, 11, 13, 5, 3, 1, 6, 7, 12, 14]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0.266254</td>\n",
       "      <td>0.022870</td>\n",
       "      <td>0.042094</td>\n",
       "      <td>0.214835</td>\n",
       "      <td>0.879316</td>\n",
       "      <td>2.907068e-01</td>\n",
       "      <td>0.194405</td>\n",
       "      <td>0.201390</td>\n",
       "      <td>6.843163e-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.539193e-14</td>\n",
       "      <td>2.552196e-16</td>\n",
       "      <td>0.617458</td>\n",
       "      <td>4.926280e-04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5967</th>\n",
       "      <td>0.463169</td>\n",
       "      <td>0.811339</td>\n",
       "      <td>0.075665</td>\n",
       "      <td>0.820674</td>\n",
       "      <td>0.631104</td>\n",
       "      <td>3.693685e-09</td>\n",
       "      <td>0.198198</td>\n",
       "      <td>0.399914</td>\n",
       "      <td>5.113201e-28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.370380e-12</td>\n",
       "      <td>2.670368e-22</td>\n",
       "      <td>0.339417</td>\n",
       "      <td>1.264174e-19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22637</th>\n",
       "      <td>0.157099</td>\n",
       "      <td>0.644026</td>\n",
       "      <td>0.055997</td>\n",
       "      <td>0.208734</td>\n",
       "      <td>0.618299</td>\n",
       "      <td>3.029250e-01</td>\n",
       "      <td>0.149662</td>\n",
       "      <td>0.125950</td>\n",
       "      <td>2.752513e-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.823941e-17</td>\n",
       "      <td>9.478544e-12</td>\n",
       "      <td>0.367772</td>\n",
       "      <td>1.475993e-24</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15499</th>\n",
       "      <td>0.467963</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.123533</td>\n",
       "      <td>0.832090</td>\n",
       "      <td>0.477357</td>\n",
       "      <td>5.460625e-11</td>\n",
       "      <td>0.102413</td>\n",
       "      <td>0.397683</td>\n",
       "      <td>6.668265e-34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.878430e-15</td>\n",
       "      <td>7.121354e-20</td>\n",
       "      <td>0.559213</td>\n",
       "      <td>3.039009e-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19906</th>\n",
       "      <td>0.196508</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.209058</td>\n",
       "      <td>0.052081</td>\n",
       "      <td>0.648811</td>\n",
       "      <td>2.910778e-01</td>\n",
       "      <td>0.815041</td>\n",
       "      <td>0.516874</td>\n",
       "      <td>9.650097e-37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.168222e-07</td>\n",
       "      <td>3.795385e-06</td>\n",
       "      <td>0.226131</td>\n",
       "      <td>1.480822e-15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  workclass    fnlwgt  education  education-num  \\\n",
       "812    0.266254   0.022870  0.042094   0.214835       0.879316   \n",
       "5967   0.463169   0.811339  0.075665   0.820674       0.631104   \n",
       "22637  0.157099   0.644026  0.055997   0.208734       0.618299   \n",
       "15499  0.467963   0.000997  0.123533   0.832090       0.477357   \n",
       "19906  0.196508   0.000003  0.209058   0.052081       0.648811   \n",
       "\n",
       "       marital-status  occupation  relationship          race  sex  \\\n",
       "812      2.907068e-01    0.194405      0.201390  6.843163e-19  1.0   \n",
       "5967     3.693685e-09    0.198198      0.399914  5.113201e-28  1.0   \n",
       "22637    3.029250e-01    0.149662      0.125950  2.752513e-18  0.0   \n",
       "15499    5.460625e-11    0.102413      0.397683  6.668265e-34  1.0   \n",
       "19906    2.910778e-01    0.815041      0.516874  9.650097e-37  1.0   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "812    4.539193e-14  2.552196e-16        0.617458    4.926280e-04     1.0  \n",
       "5967   2.370380e-12  2.670368e-22        0.339417    1.264174e-19     0.0  \n",
       "22637  5.823941e-17  9.478544e-12        0.367772    1.475993e-24     0.0  \n",
       "15499  6.878430e-15  7.121354e-20        0.559213    3.039009e-13     1.0  \n",
       "19906  1.168222e-07  3.795385e-06        0.226131    1.480822e-15     1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_data = train_decaf(dataset_train, dag_seed, biased_edges=bias_dict_ftu)\n",
    "synth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(synth_data, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECAF-CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:175: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n",
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:170: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised adjacency matrix as parsed:\n",
      " Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üò° M after fci stuff Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üò∂‚Äçüå´Ô∏è self.gen.M right after init Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üü° DENSE DAGA Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "ü•∂ generator M Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üü† M AT GEN SYNTHETIC, Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üî¥ GEN ORDER [0, 2, 4, 8, 9, 10, 11, 13, 5, 3, 1, 6, 7, 12, 14]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14048</th>\n",
       "      <td>0.188350</td>\n",
       "      <td>1.688214e-04</td>\n",
       "      <td>0.093547</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.971950</td>\n",
       "      <td>0.288652</td>\n",
       "      <td>0.087973</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>1.538513e-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.019098e-08</td>\n",
       "      <td>6.630502e-04</td>\n",
       "      <td>0.240153</td>\n",
       "      <td>4.250481e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>0.106622</td>\n",
       "      <td>1.011682e-02</td>\n",
       "      <td>0.086411</td>\n",
       "      <td>0.181665</td>\n",
       "      <td>0.563363</td>\n",
       "      <td>0.286136</td>\n",
       "      <td>0.482522</td>\n",
       "      <td>0.754618</td>\n",
       "      <td>3.104885e-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.115307e-09</td>\n",
       "      <td>2.214673e-01</td>\n",
       "      <td>0.331639</td>\n",
       "      <td>1.687875e-26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23858</th>\n",
       "      <td>0.211246</td>\n",
       "      <td>1.917130e-07</td>\n",
       "      <td>0.054367</td>\n",
       "      <td>0.030722</td>\n",
       "      <td>0.631940</td>\n",
       "      <td>0.288672</td>\n",
       "      <td>0.083053</td>\n",
       "      <td>0.703324</td>\n",
       "      <td>7.864906e-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.233765e-06</td>\n",
       "      <td>2.287603e-09</td>\n",
       "      <td>0.246762</td>\n",
       "      <td>2.196123e-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.115459</td>\n",
       "      <td>3.878504e-07</td>\n",
       "      <td>0.062410</td>\n",
       "      <td>0.034999</td>\n",
       "      <td>0.963131</td>\n",
       "      <td>0.270482</td>\n",
       "      <td>0.316779</td>\n",
       "      <td>0.041284</td>\n",
       "      <td>3.299533e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.061675e-04</td>\n",
       "      <td>1.129459e-22</td>\n",
       "      <td>0.553476</td>\n",
       "      <td>2.504910e-19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>0.561209</td>\n",
       "      <td>1.120683e-01</td>\n",
       "      <td>0.078687</td>\n",
       "      <td>0.858158</td>\n",
       "      <td>0.488480</td>\n",
       "      <td>0.457073</td>\n",
       "      <td>0.220861</td>\n",
       "      <td>0.987832</td>\n",
       "      <td>4.819847e-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.967876e-03</td>\n",
       "      <td>1.740339e-01</td>\n",
       "      <td>0.314620</td>\n",
       "      <td>9.778274e-22</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age     workclass    fnlwgt  education  education-num  \\\n",
       "14048  0.188350  1.688214e-04  0.093547   0.041265       0.971950   \n",
       "14590  0.106622  1.011682e-02  0.086411   0.181665       0.563363   \n",
       "23858  0.211246  1.917130e-07  0.054367   0.030722       0.631940   \n",
       "780    0.115459  3.878504e-07  0.062410   0.034999       0.963131   \n",
       "15547  0.561209  1.120683e-01  0.078687   0.858158       0.488480   \n",
       "\n",
       "       marital-status  occupation  relationship          race  sex  \\\n",
       "14048        0.288652    0.087973      0.326100  1.538513e-30  1.0   \n",
       "14590        0.286136    0.482522      0.754618  3.104885e-15  1.0   \n",
       "23858        0.288672    0.083053      0.703324  7.864906e-26  1.0   \n",
       "780          0.270482    0.316779      0.041284  3.299533e-07  0.0   \n",
       "15547        0.457073    0.220861      0.987832  4.819847e-23  0.0   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "14048  9.019098e-08  6.630502e-04        0.240153    4.250481e-09     1.0  \n",
       "14590  1.115307e-09  2.214673e-01        0.331639    1.687875e-26     0.0  \n",
       "23858  4.233765e-06  2.287603e-09        0.246762    2.196123e-11     0.0  \n",
       "780    6.061675e-04  1.129459e-22        0.553476    2.504910e-19     1.0  \n",
       "15547  1.967876e-03  1.740339e-01        0.314620    9.778274e-22     1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_data = train_decaf(dataset_train, dag_seed, biased_edges=bias_dict_cf)\n",
    "synth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7714895888948211,\n",
       " 'recall': 0.9607712765957447,\n",
       " 'auroc': 0.5489340253946465,\n",
       " 'dp': 0.026723623,\n",
       " 'ftu': 0.016499996}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(synth_data, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECAF-DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:175: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n",
      "/Users/alyssaunell/UvA_FACT2022/venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:170: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised adjacency matrix as parsed:\n",
      " Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üò° M after fci stuff Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üò∂‚Äçüå´Ô∏è self.gen.M right after init Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üü° DENSE DAGA Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "ü•∂ generator M Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üü† M AT GEN SYNTHETIC, Parameter containing:\n",
      "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "üî¥ GEN ORDER [0, 2, 4, 8, 9, 10, 11, 13, 5, 3, 1, 6, 7, 12, 14]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14048</th>\n",
       "      <td>0.178706</td>\n",
       "      <td>6.868240e-01</td>\n",
       "      <td>0.073524</td>\n",
       "      <td>0.039970</td>\n",
       "      <td>0.483234</td>\n",
       "      <td>2.923991e-01</td>\n",
       "      <td>0.364860</td>\n",
       "      <td>0.600087</td>\n",
       "      <td>9.417156e-18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.695503e-13</td>\n",
       "      <td>4.510889e-08</td>\n",
       "      <td>0.200148</td>\n",
       "      <td>1.051372e-19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>0.303430</td>\n",
       "      <td>6.950489e-08</td>\n",
       "      <td>0.185627</td>\n",
       "      <td>0.054157</td>\n",
       "      <td>0.498580</td>\n",
       "      <td>2.931322e-01</td>\n",
       "      <td>0.250863</td>\n",
       "      <td>0.343016</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.053533e-04</td>\n",
       "      <td>2.480250e-12</td>\n",
       "      <td>0.439412</td>\n",
       "      <td>2.627585e-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23858</th>\n",
       "      <td>0.274386</td>\n",
       "      <td>6.018996e-03</td>\n",
       "      <td>0.069567</td>\n",
       "      <td>0.143423</td>\n",
       "      <td>0.562086</td>\n",
       "      <td>1.858769e-09</td>\n",
       "      <td>0.098196</td>\n",
       "      <td>0.440008</td>\n",
       "      <td>1.494615e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.212908e-13</td>\n",
       "      <td>1.558180e-17</td>\n",
       "      <td>0.733754</td>\n",
       "      <td>9.311341e-27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.324166</td>\n",
       "      <td>4.213441e-01</td>\n",
       "      <td>0.072413</td>\n",
       "      <td>0.607543</td>\n",
       "      <td>0.479265</td>\n",
       "      <td>1.115209e-03</td>\n",
       "      <td>0.650338</td>\n",
       "      <td>0.178754</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.986288e-17</td>\n",
       "      <td>1.686280e-09</td>\n",
       "      <td>0.355282</td>\n",
       "      <td>4.801889e-27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>0.418044</td>\n",
       "      <td>2.367120e-02</td>\n",
       "      <td>0.131401</td>\n",
       "      <td>0.074534</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>2.169359e-06</td>\n",
       "      <td>0.178980</td>\n",
       "      <td>0.403039</td>\n",
       "      <td>5.368275e-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.450151e-18</td>\n",
       "      <td>3.589154e-10</td>\n",
       "      <td>0.372395</td>\n",
       "      <td>1.145330e-15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age     workclass    fnlwgt  education  education-num  \\\n",
       "14048  0.178706  6.868240e-01  0.073524   0.039970       0.483234   \n",
       "14590  0.303430  6.950489e-08  0.185627   0.054157       0.498580   \n",
       "23858  0.274386  6.018996e-03  0.069567   0.143423       0.562086   \n",
       "780    0.324166  4.213441e-01  0.072413   0.607543       0.479265   \n",
       "15547  0.418044  2.367120e-02  0.131401   0.074534       0.535354   \n",
       "\n",
       "       marital-status  occupation  relationship          race  sex  \\\n",
       "14048    2.923991e-01    0.364860      0.600087  9.417156e-18  1.0   \n",
       "14590    2.931322e-01    0.250863      0.343016  0.000000e+00  1.0   \n",
       "23858    1.858769e-09    0.098196      0.440008  1.494615e-01  1.0   \n",
       "780      1.115209e-03    0.650338      0.178754  0.000000e+00  0.0   \n",
       "15547    2.169359e-06    0.178980      0.403039  5.368275e-17  1.0   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "14048  4.695503e-13  4.510889e-08        0.200148    1.051372e-19     1.0  \n",
       "14590  4.053533e-04  2.480250e-12        0.439412    2.627585e-21     0.0  \n",
       "23858  1.212908e-13  1.558180e-17        0.733754    9.311341e-27     1.0  \n",
       "780    2.986288e-17  1.686280e-09        0.355282    4.801889e-27     1.0  \n",
       "15547  7.450151e-18  3.589154e-10        0.372395    1.145330e-15     1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_data = train_decaf(dataset_train, dag_seed, biased_edges=bias_dict_dp)\n",
    "synth_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7578729994837378,\n",
       " 'recall': 0.976063829787234,\n",
       " 'auroc': 0.515249656829101,\n",
       " 'dp': 0.007920206,\n",
       " 'ftu': 0.00049996376}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(synth_data, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "183d8cb6150ac0256029b844d73acda4711655573a37b5ae608e6daef3686e31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
